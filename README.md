# Domain-specific-content-generation
<a name="Оглавление"></a>
## Оглавление
- [Изучение статьи](#Изучениестатьи)
     - [Что это такое?](#Чтоэтотакое?)
     - [Почему над этим работают?](#Почемунадэтимработают?)
     - [Как формулируется задача?](#Какформулируетсязадача?)
     - [В чем ее основная идея?](#Вчемееосновнаяидея?)
     - [В чем ее новаторство?](#Вчемееноваторство?)
     - [Какие получились результаты?](#Какиеполучилисьрезультаты?)
- [Практическая часть](#Практическаячасть)
     - [Результаты с картинками](#Результатыскартинками)
     - [Наблюдения](#Наблюдения)
     - [... и проблемы](#проблемы)
- [Запуск](#Запуск)

<a name="Изучениестатьи"></a>
## Изучение статьи
<a name="Чтоэтотакое?"></a>
### Что это такое?
Это новый подход, который позволяет генерировать изображения объектов в различных контекстах.  
[Оглавление](#Оглавление)
<a name="Почемунадэтимработают?"></a>
### Почему над этим работают?
Это уникальное решение. Уже существуют нейронные сети, которые позволяют генерировать изображения по текстовому описанию, но это всегда будут разные объекты. Например, если попросить изобразить собаку конкретной породы, в конкретной позе на фоне эйфелевой башни - это всегда будут разные собаки. Представленное решение способно на основе 3-5 фотографий одной собаки генерировать изображения с ней в разных контекстов с сохранением отличительных черт и естественности. Современные модели не могут точно реконструировать внешний вид заданных предметов, а лишь создают вариации содержания изображения.  
[Оглавление](#Оглавление)
<a name="Какформулируетсязадача?"></a>
### Как формулируется задача?
Необходимо создать модель, которая по входящим 3-5 изображениям одного и того же объекта будет генерировать его в различных контекстах, связать изображение и текст так, чтобы сохранить уникальные черты объекта и не потерять естественность.  
[Оглавление](#Оглавление)
<a name="Вчемееосновнаяидея?"></a>
### В чем ее основная идея?
В данной работе исследователи представляют новый подход к "персонализации" диффузионных моделей "текст-изображение" (их адаптации к потребностям пользователя при генерации изображений). Цель - расширить языковой словарь модели таким образом, чтобы она связывала новые слова с конкретными темами, которые пользователь хочет генерировать. Для этого предлагается методика представления заданного субъекта с помощью редких токенов-идентификаторов и тонкуая настройка предварительно обученного фреймворка преобразования текста в изображение на основе диффузии. Тонкая настройка модели преобразования текста в изображение осуществляется с помощью входных изображений и текстовых подсказок, содержащих уникальный идентификатор, за которым следует название класса предмета (например, "A [V] dog").  
[Оглавление](#Оглавление)
<a name="Вчемееноваторство?"></a>
### В чем ее новаторство?
Это иновационное решение, которое позволяет решить новую сложную задачу генерации изображений на основе субъекта. Для предотвращения явления, при котором модель ассоциирует название класса с конкретным экземпляром, предлагается использовать автогенную функцию потерь. Также данный подход позволяет решать множество новых задач: реконтекстуализацию, художественную рендеризацию, синтез новых представлений, модификацию свойств.  
[Оглавление](#Оглавление)
<a name="Какиеполучилисьрезультаты?"></a>
### Какие получились результаты?
Авторы представили пример генерации изображения с несколькими входными изображениями на дополнительном контексте, получилось очень точно все воспроизвести. Был создан датасет, состоящий из 30 различных классов из животных и неодушевленных объектов. Было проведено сравнение результатов с Textual Inversion (единственной сопоставимой работе, которая ориентирована на предмет и текст и генерирует изображения), DreamBooth (Imagen) и DreamBooth (Stable Diffusion). По всем метрикам (DINO, CLIP-I, CLIP-T) результаты DreamBooth (Imagen) оказались более приближены к реальным изображениям. Также авторы говорят об ограничениях данного метода, приводят несколько неудачных примеров генерации. Возможные причины - слабый приоритет для таких контекстов, сложность генерации как предмета, так и заданного понятия вместе из-за низкой вероятности совместного использования. Вторая причина - смешение контекста и внешнего вида, когда внешний вид субъекта изменяется под воздействием контекста подсказки, например с изменением цвета объекта. Третья - чрезмерную подгонка к реальным изображениям, которая происходит, когда подсказка похожа на исходную обстановку, в которой находится объект. Было замечено, что такие предметы, как рюкзак легче поддаются генерации, чем кошки и собаки.  
[Оглавление](#Оглавление)
<a name="Практическаячасть"></a>
## Практическая часть
<a name="Результатыскартинками"></a>
### Результаты с картинками
В процессе работы получилось дообучить модель на небольшом домене из 10 картинок с героем Cinnamonroll. Примеры картинок:  
![ad83a44ef00c2a538ab0c4c8594703c2](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/346379a9-9b78-496f-ab99-44f160bbb635)
![68d3b63a3fcc590ac994505472f4f5e0](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/52dc5969-60b1-4580-b637-f5116a0f8046)  
  
Было решено (частично методом проб и ошибок, частично логически) увеличить количество эпох до 500 и изменить batch size на 10, learning rate для энкодера = 5e-5, random seed = 40. В качестве INFERENCE_PROMPT для тестирования использовалась фраза "cinnamoroll a toy". Получился следующий результат:  
![загруженное (16)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/5fee849f-3bba-4701-af94-650d4bb974a3)
![загруженное (4)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/dda15d97-eded-492a-ba5e-d17d7ee84b43)    
Для "cinnamoroll ridding a bycicle" получаются пугающие огромные зайцы на настоящих велосипедах, не как на картинке из обучающего набора:  
![загруженное (15)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/f547699e-b4e6-4692-b35d-a9a287953727)
![загруженное (14)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/b05f0857-05b6-41c2-b7c3-fab90a47dbff)    
Для "crochet toy cinnamoroll" получаются лунтики:  
![загруженное (5)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/e3d9c4a0-95e0-4338-adbd-a14510261fb9)
![загруженное (9)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/2121eda3-7d01-4b24-9aa8-cb2cc3154e78)
![загруженное (13)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/a70ea377-f1bf-49fa-90eb-dab510c49c70)    
Для "cup with printed cinnamoroll" самые красивые изображения(я бы хотела такую кружку):  
![загруженное (2)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/f9d2301b-c6c7-4a32-848b-b54320e9617a)
![загруженное (1)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/1ee545e9-d14c-4348-a53e-a5bbfe361e8c)    
Для "art of cinnamoroll eating strawberry" тоже лунтики:  
![загруженное (7)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/7d15432e-54e7-46ac-8f1d-132126601362)
![загруженное (6)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/e69911c4-92b4-4477-a1a2-ef0dc56a169b)    
Для "art of cooking cinnamoroll" получается такой mood ▓▒░(°◡°)░▒▓:  
![загруженное (3)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/7e0f3311-80ae-4f1b-9bcd-beecdd320f50)    
    
При меньшем количестве эпох получались совсем невнятные изображения, например при epoch = 300, batch size = 1, learning rate для энкодера = 1e-4, prompt = 'cinnamoroll a toy':  
![загруженное](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/27d3cc75-a605-428b-b15d-6286f4328f62)    
Или другой пример epoch = 300, batch size = 1, learning rate для энкодера = 1e-4, prompt = 'crochet toy cinnamoroll':  
![загруженное (8)](https://github.com/compfee/Domain-specific-content-generation/assets/55783463/a138f58c-ab8e-4f99-aba3-7754b208addb)  
  [Оглавление](#Оглавление)
<a name="Наблюдения"></a>
### Наблюдения
- Самые лучшие изображения получаются с какими то простыми объектами, например с кружками.
- Сеть выдает более правдивые(если так можно судить) результаты, если выкрутить GUIDANCE на 9-12.
- При большем количестве эпох результаты не становятся сильно лучше, чем при 500, если увеличивать learning rate, то само собой результаты ухудшаются.
- Решила посмотреть, как дообучают модель другие люди, в случае добавления текстовых файлов с ключевыми словами результаты генерации оказываются лучше.
[Оглавление](#Оглавление)
<a name="проблемы"></a>
### ... и проблемы
В самом начале я столкнулась с ошибкой при запуске обучения:  

>TypeError: Accelerator.__init__() got an unexpected keyword argument 'logging_dir'

Оказалось что в файле train_lora_dreambooth.py необходимо заменить logging_dir на project_dir т. к. [версия устарела](https://github.com/huggingface/accelerate/issues/1550#issuecomment-1580974666)  

А в самом конце у меня закончились вычислительные ресурсы в коллабе)))  
[Оглавление](#Оглавление)
<a name="Запуск"></a>
## Запуск
- С помощью  [Google Colab](https://colab.research.google.com/drive/1KSWUJNQqrKeELghrk3esOyG6FdGI4HEn?usp=sharing) запустить ноутбук.
- Создать папки input и output
- Указать их в качестве OUTPUT_DIR и IMAGES_FOLDER_OPTIONAL, а также все learning rate
- Загрузить в input изображения
- Предварительно перед запуском обучения заменить 493 строчку в файле /content/lora/training_scripts/train_lora_dreambooth.py.  

Было:  
```python
accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with="tensorboard",
        logging_dir=logging_dir,
    )
```  
Стало:  
```python
accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with="tensorboard",
        project_dir=logging_dir,
    )
```
Готово! °˖✧◝(⁰▿⁰)◜✧˖°  
[Оглавление](#Оглавление)








